{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f040e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FOLDER = '../results/predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f8ada",
   "metadata": {},
   "source": [
    "### NRMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b555017",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = ['sp', 'ce', 'rs']\n",
    "METRIC = 'nrmse'  #nmae\n",
    "\n",
    "def compute_metric(y_true, y_pred, metric='nrmse'):\n",
    "    if metric == 'nrmse':\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        return round((rmse / y_true.mean()) * 100, 2)\n",
    "    elif metric == 'nmae':\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        return round((mae / y_true.mean()) * 100, 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "\n",
    "def collect_baselines(folder, suffix, metric):\n",
    "    results = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(f'{suffix}.csv'):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df.columns = df.columns.str.strip()\n",
    "            try:\n",
    "                val = compute_metric(df['y_test'], df[f'y_predict_{suffix}'], metric)\n",
    "                key = file.replace(f'_{suffix}.csv', '.csv')\n",
    "                results[key] = val\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return results\n",
    "\n",
    "def evaluate_models(folder, metric):\n",
    "    results = []\n",
    "    models_suffix = {\n",
    "        'holtwinters': collect_baselines(folder, 'HoltWinters', metric),\n",
    "        'gru': collect_baselines(folder, 'GRU', metric),\n",
    "        'arima': collect_baselines(folder, 'ARIMA', metric)\n",
    "    }\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if (not file.endswith('.csv')) or any(file.endswith(f) for f in ['GRU.csv', 'ARIMA.csv', 'HoltWinters.csv']):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(os.path.join(folder, file))\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        if 'y_test' not in df.columns or 'y_predict_StackingRegressor' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        destination = next((d for d in TARGETS if f\"-{d}\" in file), None)\n",
    "        if destination is None:\n",
    "            continue\n",
    "\n",
    "        y_true = df['y_test'].dropna()\n",
    "        if y_true.empty:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            stacking = compute_metric(y_true, df['y_predict_StackingRegressor'], metric)\n",
    "            if stacking >= 10:\n",
    "                continue\n",
    "\n",
    "            values = {\n",
    "                'Pop': file.replace('Vazao_', '').replace('.csv', '').upper(),\n",
    "                'destination': destination,\n",
    "                f'{metric}_stacking': stacking,\n",
    "                f'{metric}_arima': models_suffix['arima'].get(file),\n",
    "                f'{metric}_gru': models_suffix['gru'].get(file),\n",
    "                f'{metric}_holtwinters': models_suffix['holtwinters'].get(file)\n",
    "            }\n",
    "            results.append(values)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_metrics = evaluate_models(CSV_FOLDER, METRIC)\n",
    "df_metrics = df_metrics.sort_values(f'{METRIC}_stacking')\n",
    "metric_cols = [col for col in df_metrics.columns if col.startswith(METRIC)]\n",
    "df_summary = df_metrics[['Pop'] + metric_cols]\n",
    "\n",
    "overall = df_summary[metric_cols].mean().to_dict()\n",
    "overall['Pop'] = 'overall'\n",
    "df_summary = pd.concat([df_summary, pd.DataFrame([overall])], ignore_index=True)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd104f",
   "metadata": {},
   "source": [
    "### Fuzzy Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8900a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS_MBPS = np.arange(0, 2100, 100)\n",
    "LABELS = [f'{BINS_MBPS[i]}-{BINS_MBPS[i+1]}' for i in range(len(BINS_MBPS)-1)]\n",
    "\n",
    "def fuzzy_weight(distance):\n",
    "    return {0: 1.0, 1: 0.75, 2: 0.5}.get(distance, 0.0)\n",
    "\n",
    "def compute_fuzzy_accuracy(y_true, y_pred):\n",
    "    df = pd.DataFrame({'y_test': y_true, 'y_pred': y_pred}).dropna()\n",
    "    if df.empty:\n",
    "        return None, 0\n",
    "\n",
    "    df['y_test_mbps'] = df['y_test'] / 1e6\n",
    "    df['y_pred_mbps'] = df['y_pred'] / 1e6\n",
    "\n",
    "    df['true_bin'] = pd.cut(df['y_test_mbps'], bins=BINS_MBPS, labels=LABELS, right=False)\n",
    "    df['pred_bin'] = pd.cut(df['y_pred_mbps'], bins=BINS_MBPS, labels=LABELS, right=False)\n",
    "\n",
    "    label_to_index = {label: idx for idx, label in enumerate(LABELS)}\n",
    "    df['true_idx'] = df['true_bin'].map(label_to_index)\n",
    "    df['pred_idx'] = df['pred_bin'].map(label_to_index)\n",
    "\n",
    "    df = df.dropna(subset=['true_idx', 'pred_idx'])\n",
    "    if df.empty:\n",
    "        return None, 0\n",
    "\n",
    "    df['distance'] = (df['true_idx'].astype(int) - df['pred_idx'].astype(int)).abs()\n",
    "    df['fuzzy_weight'] = df['distance'].apply(fuzzy_weight)\n",
    "\n",
    "    return df['fuzzy_weight'].mean(), len(df)\n",
    "\n",
    "def load_predictions(base_file, folder):\n",
    "    df_base = pd.read_csv(os.path.join(folder, base_file))\n",
    "    df_base.columns = df_base.columns.str.strip()\n",
    "\n",
    "    if 'y_test' not in df_base.columns:\n",
    "        return None, {}\n",
    "\n",
    "    y_test = df_base['y_test']\n",
    "    predictions = {\n",
    "        'StackingRegressor': df_base.get('y_predict_StackingRegressor'),\n",
    "        'KNeighborsRegressor': df_base.get('y_predict_KNeighborsRegressor'),\n",
    "        'GradientBoostingRegressor': df_base.get('y_predict_GradientBoostingRegressor'),\n",
    "        'XGBRegressor': df_base.get('y_predict_XGBRegressor'),\n",
    "        'RandomForestRegressor': df_base.get('y_predict_RandomForestRegressor'),\n",
    "        'ElasticNet': df_base.get('y_predict_ElasticNet')\n",
    "    }\n",
    "\n",
    "    for model_name in ['GRU', 'ARIMA', 'HoltWinters']:\n",
    "        model_file = base_file.replace('.csv', f'_{model_name}.csv')\n",
    "        model_path = os.path.join(folder, model_file)\n",
    "        if os.path.exists(model_path):\n",
    "            df_model = pd.read_csv(model_path)\n",
    "            df_model.columns = df_model.columns.str.strip()\n",
    "            col_pred = f'y_predict_{model_name}'\n",
    "            if col_pred in df_model.columns:\n",
    "                predictions[model_name] = df_model[col_pred]\n",
    "\n",
    "    return y_test, predictions\n",
    "\n",
    "def evaluate_fuzzy_accuracy(folder):\n",
    "    results = []\n",
    "    base_files = [f for f in os.listdir(folder)\n",
    "                  if f.endswith('.csv') and not any(f.endswith(suf) for suf in ['GRU.csv', 'ARIMA.csv', 'HoltWinters.csv'])]\n",
    "\n",
    "    for file in base_files:\n",
    "        y_test, predictions = load_predictions(file, folder)\n",
    "        if y_test is None:\n",
    "            continue\n",
    "\n",
    "        pop_name = file.replace('Vazao_', '').replace('.csv', '').upper()\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            if y_pred is None:\n",
    "                continue\n",
    "\n",
    "            accuracy, sample_count = compute_fuzzy_accuracy(y_test, y_pred)\n",
    "            if accuracy is not None:\n",
    "                results.append({\n",
    "                    'Pop': pop_name,\n",
    "                    'Model': model_name,\n",
    "                    'Fuzzy_Accuracy': accuracy * 100,\n",
    "                    'Samples': sample_count\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_fuzzy = evaluate_fuzzy_accuracy(CSV_FOLDER)\n",
    "df_pivot = df_fuzzy.pivot(index='Pop', columns='Model', values='Fuzzy_Accuracy').reset_index()\n",
    "df_counts = df_fuzzy.groupby('Pop')['Samples'].sum().reset_index()\n",
    "df_result = pd.merge(df_pivot, df_counts, on='Pop').round(2)\n",
    "\n",
    "mean_values = df_result.drop(columns=['Pop']).mean()\n",
    "overall_row = {**mean_values.to_dict(), 'Pop': 'overall'}\n",
    "df_result = pd.concat([df_result, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "selected_columns = ['Pop', 'StackingRegressor', 'ARIMA', 'GRU', 'HoltWinters']\n",
    "\n",
    "df_result[selected_columns]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
